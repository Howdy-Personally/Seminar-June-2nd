# Seminr-June-2nd

## content
### 一、NLP Transformer
### 二、Visual Transformer
### 三、Detection Transformer
### 四、My Work


## Introduction
### 1. NLP Transformer
Transformer由论文《Attention is All You Need》提出
这是论文的链接 https://arxiv.org/abs/1706.03762
Transformer改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。并且Transformer可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。
![avater](https://github.com/Howdy-Personally/Seminar-June-2nd/blob/main/pic/TheTransformerModelArchitecture.png)
#### 从宏观的视角开始
在机器翻译中，就是输入一种语言，输出另一种语言。
![avater](https://github.com/Howdy-Personally/Seminar-June-2nd/blob/main/pic/pic1.jpg)
![avater](https://github.com/Howdy-Personally/Seminar-June-2nd/blob/main/pic/pic2.jpg)
![avater](https://github.com/Howdy-Personally/Seminar-June-2nd/blob/main/pic/pic3.jpg)
![avater](https://github.com/Howdy-Personally/Seminar-June-2nd/blob/main/pic/pic4.jpg)
![avater](https://github.com/Howdy-Personally/Seminar-June-2nd/blob/main/pic/pic5.jpg)

### 二、Visual Transformer
### 三、Detection Transformer
### 四、我的工作

